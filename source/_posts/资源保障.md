---
title: Linux系统下的系统资源和网络资源控制
date: 2021-05-30 00:00:00
categories:
    - 学习
tags:
    - Linux
    - 系统管理
    - 网络管理
---
## 今日珠宝

宝格丽BVLGARI系列戒指

18K玫瑰金材质镶嵌7颗钻石，非常经典优雅的设计风格。指圈大小63号（正品实拍）

![宝格丽BVLGARI系列戒指](https://pic1.zhimg.com/80/v2-a78042ef069ac261a2b24c645c26204c_720w.jpg)

原文为[知乎](https://zhuanlan.zhihu.com/p/376501404)，现转移到个人博客。
<a href="https://zhuanlan.zhihu.com/p/376501404" target="_blank" class="LinkCard">知乎原文</a>

# Linux系统下的系统资源和网络资源控制

众所周知，在互联网诞生之初都是各个高校和科研机构相互通讯，并没有网络流量控制方面的考虑和设计，IP协议的原则是尽可能好地为所有数据流服务，不同的数据流之间是平等的。然而多年的实践表明，这种原则并不是最理想的，有些数据流应该得到特别的照顾， 比如，远程登录的交互数据流应该比数据下载有更高的优先级。

同样，我们希望操作系统内的资源也是不平等的，我们总是希望某些进程占用更多的资源，以满足我们当前最迫切的需求，比如看电影的时候，我们希望视频播放器更多的资源，而不希望这个时候浏览器占用过多的资源。

因此细粒度的对操作系统资源的控制是我们每一个人都想做的事，自己的地盘自己做主。

## 限制系统资源

在Linux上限制系统资源主要使用的工具是`cgroup`.

基本语法不在赘述，相关文章很多`见参考资料`。我们想限制的系统资源主要是CPU，内存还有IO设备(例如，硬盘读写速度)。在Linux系统中，这些资源的分配和管理都由相应目录`/cgroup/cpu/,/cgroup/memory ,/cgroup/blkio`下配置。下面举例进行说明：例如你想建一个cpu控制策略，取名为`foo`,则你只需要在目录`/cgroup/cpu/`中`mkdir foo`，操作系统会为你创造一些列文件，就可以用操作文件的方式控制你的系统了。如下例：

### **cpu限制实例**

```shell
1. [root@localhost /]# mkdir -p /cgroup/cpu/foo/ 
2. [root@localhost /]# mkdir -p /cgroup/cpuset/foo/ 
3. [root@localhost /]# echo 50000 > /cgroup/cpu/foo/cpu.cfs_quota_us 
4. [root@localhost /]# echo 100000 > /cgroup/cpu/foo/cpu.cfs_period_us 
5. [root@localhost /]# echo "0" > /cgroup/cpuset/foo/cpuset.mems 
6. [root@localhost /]# echo "1" > /cgroup/cpuset/foo/cpuset.cpus 
7. [root@localhost /]# echo 28819 > /cgroup/cpu/foo/tasks  
```



### **内存限制实例**

```shell
1. [root@localhost /]# mkdir -p /cgroup/memory/foo 
2. [root@localhost /]# echo > /cgroup/memory/foo/memory.limit_in_bytes 
3. [root@localhost /]# echo 44476 > /cgroup/memory/foo/tasks  
```

> 内存参数
>
> ```shell
>  cgroup.event_control       #用于eventfd的接口
>  memory.usage_in_bytes      #显示当前已用的内存
>  memory.limit_in_bytes      #设置/显示当前限制的内存额度
>  memory.failcnt             #显示内存使用量达到限制值的次数
>  memory.max_usage_in_bytes  #历史内存最大使用量
>  memory.soft_limit_in_bytes #设置/显示当前限制的内存软额度
>  memory.stat                #显示当前cgroup的内存使用情况
>  memory.use_hierarchy       #设置/显示是否将子cgroup的内存使用情况统计到当前cgroup里面
>  memory.force_empty         #触发系统立即尽可能的回收当前cgroup中可以回收的内存
>  memory.pressure_level      #设置内存压力的通知事件，配合cgroup.event_control一起使用
>  memory.swappiness          #设置和显示当前的swappiness
>  memory.move_charge_at_immigrate #设置当进程移动到其他cgroup中时，它所占用的内存是否也随着移动过去
>  memory.oom_control         #设置/显示oom controls相关的配置
>  memory.numa_stat           #显示numa相关的内存
> ```
>
> 

### **IO限制实例**

```shell
1. [root@localhost ~]# mkdir -p /cgroup/blkio/foo 
2. [root@localhost ~]# echo '8:0  10485760' > /cgroup/blkio/foo/blkio.throttle.read_bps_device 
3. [root@localhost ~]# echo 45033 > /cgroup/blkio/foo/tasks 
#  注2：8:0对应主设备号和副设备号，可以通过ls -l /dev/sda查看
```



### 一个限制内存和CPU的具体例子

之前在学习cgroup的时候，较多的都是减少原理和一些简单的例子，但这对直观的取理解cgroup还是有点障碍，所以学习的过程中，参考他人的博客结合自己的理解写了一个例子，供大家参考。

例子分为两个部分

#### 0x01 被限制应用

被限制应用是一个CPU消耗型，具体见下面代码

```c
int main(void)
{
    int i = 0;
    for(;;) i++;
    return 0;
}
```

直接编译，假如这里编译出的程序名为`deadloop`, `top`查看如下：

```shell
  PID USER      PR  NI    VIRT    RES    SHR S  %CPU %MEM     TIME+ COMMAND     
 31404 root  20   0    4220    656    588 R  98.7  0.0   8:45.27 deadloop
```



#### 0x02 限制规则

这里我们做两个方面的工作：限制CPU使用率和限制内存。具体代码如下：

```shell
# 增加规则代码
#!/bin/bash
# 规则集合在testdead
gname='testdead'
value=50000
prog=$1
mem_val=1024000

rmdir /sys/fs/cgroup/cpu/${gname};
rmdir /sys/fs/cgroup/memory/$gname;

mkdir /sys/fs/cgroup/cpu/$gname;
mkdir /sys/fs/cgroup/memory/$gname;

echo $value | sudo tee /sys/fs/cgroup/cpu/$gname/cpu.cfs_quota_us;
echo $mem_val | sudo tee /sys/fs/cgroup/memory/$gname/memory.limit_in_bytes;

for x in $(pidof $prog)
do 
	echo $x  | sudo tee /sys/fs/cgroup/cpu/$gname/tasks;
	echo $x | sudo tee /sys/fs/cgroup/memory/$gname/tasks

	echo 0 | sudo tee /sys/fs/cgroup/memory/$gname/memory.swappiness
done
```

```shell
# 删除规则代码
#!/bin/bash

dname=testdead

cgdelete cpu:$dname
```

上述规则应用后，`top`查看如下：

```shell
 31404 root  20   0    4220    656    588 R  49.8  0.0   8:08.46 deadloop   
```

可以看出，两次的CPU使用率不一样，显示出了cgroup的限制效果。



## 限制网络资源

### 背景

Linux操作系统中，报文分组从输入网卡(入口)接收进来，经过路由的查找， 以确定是发给本机的，还是需要转发的。如果是发给本机的，就直接向上递交给上层的协议，比如TCP，如果是转发的， 则会从输出网卡(出口)发出。网络流量的控制通常发生在输出网卡处。虽然在路由器的入口处也可以进行流量控制，Linux也具有相关的功能， 但一般说来， 由于我们无法控制自己网络之外的设备， 入口处的流量控制相对较难。本文将集中介绍出口处的流量控制。

### 相关概念

流量控制（`Traffic Control`， `tc`）是`Linux`内核提供的流量限速、整形和策略控制机制。它以`qdisc-class-filter`的树形结构来实现对流量的分层控制

在Linux中，流量控制都是通过TC这个工具来完成的。通常， 要对网卡进行流量控制的配置，需要进行如下的步骤:

- 为网卡配置一个队列;

- 在该队列上建立分类;

- 根据需要建立子队列和子分类;

- 为每个分类建立过滤器。

![Traffic Control](资源保障/tc2.jpeg)

### 基本组成

从上图中可以看到，tc由`qdisc`、`fitler`和`class`三部分组成：

- `qdisc`通过队列将数据包缓存起来，用来控制网络收发的速度

  - 常见的classless qdisc有：choke, codel, p/bfifo,fq, fq_codel, gred, hhf, ingress,mqprio, multiq, netem, pfifo_fast, pie, red, rr, sfb, sfq, tbf。linux默认使用的就是fifo_fast。
  - 常见的classful qdisc有：ATM, CBQ, DRR, DSMARK, HFSC, HTB, PRIO, QFQ

- `class`用来表示控制策略

  - 分类只存在于可分类排队规则（classful qdisc）（例如，HTB和CBQ）中。

  - > **default** 这是HTB排队规则的一个可选参数，默认值为0， 当值为0时意味着会绕过所有和rootqdisc相关联的分类，然后以最大的速度出队任何未分类的流量。 
    >
    > **rate** 这个参数用来设置流量发送的最小期望速率。这个速率可以被当作承诺信息速率(CIR), 或者给某个叶子分类的保证带宽。 
    >
    > **ceil** 这个参数用来设置流量发送的最大期望速率。租借机制将会决定这个参数的实际用处。 这个速率可以被称作“突发速率”。 
    >
    > **burst** 这个参数是rate桶的大小(参见令牌桶这一节)。HTB将会在更多令牌到达之前将burst个字节的数据包出队。 
    >
    > **cburst** 这个参数是ceil桶的大小（参见令牌桶这一节）。HTB将会更多令牌(ctoken)到达之前将cburst个字节的数据包出队。 
    >
    > **quantum** 这个是HTB控制租借机制的关键参数。正常情况下，HTB自己会计算合适的quantum值，而不是由用户来设定。对这个值的轻微调整都会对租借和整形造成巨大的影响，因为HTB不仅会根据这个值向各个子分类分发流量(速率应高于rate，小于ceil)，还会根据此值输出各个子分类中的数据。 
    >
    > **r2q** 通常,quantum 的值由HTB自己计算，用户可以通过此参数设置一个值来帮助HTB为某个分类计算一个最优的quantum值。

- `filter`用来将数据包划分到具体的控制策略中

  - 包含了一个分类器实现，常见的分类器如u32，u32分类器可以允许用户基于数据包的属性来选择数据包。

语法不再赘述，相关参考资料里有。

### 入流量调度

Linux中的QoS分为入口(Ingress)部分和出口(Egress)部分，入口部分主要用于进行入口流量限速(policing)，出口部分主要用于队列调度(queuing scheduling)。

大多数排队规则(qdisc)都是用于输出方向的，输入方向只有一个排队规则，即ingress qdisc。ingress qdisc本身的功能很有限，但可用于重定向incoming packets。通过Ingress qdisc把输入方向的数据包重定向到虚拟设备ifb，而ifb的输出方向可以配置多种qdisc，就可以达到对输入方向的流量做队列调度的目的。

![输入流量调度示意](资源保障/1354528849_1019.png)

IFB说明：

> IFB — Intermediate Functional Block device。
> Q: How can we use qdisc (e.g., netem) on incoming traffic?
> A: You need to use IFB. This network device allows attaching queueing disciplines to incoming packets.
> To use an IFB, you must have IFB support in your kernel (configuration option CONFIG_IFB). Assuming that you have a modular kernel, the name of the IFB module is ifb and may be loaded using the command `modprobe ifb` (if you have modprobe installed) or `insmod /path/to/module/ifb`.
>
> ```shell
> ip link set ifb0 up
> ip link set ifb1 up
> ```
>
> By default, two IFB devices(ifb0 and ifb1) are created.
> IFB allows for queueing incoming traffic for shaping instead of dropping.
>
> 参考资料:https://blog.csdn.net/eydwyz/article/details/53392227

### 一个例子

```shell
#!/bin/bash
#fb驱动并创建ifb网卡(使用ifconfig -a 如果看到已有则无需该步骤)
#分别为出设备和入设备
outdevice=ens33
indevice=ifb0

modprobe ifb numifbs=1
# up网卡
ip link set dev $indevice up

# ------
	# 清除原有的根队列(根据实际情况操作,非必要) 
	tc qdisc del dev $outdevice root 2>/dev/null
	tc qdisc del dev $outdevice ingress 2>/dev/null
	tc qdisc del dev $indevice root 2>/dev/null
 
	#  将$outdevice的ingress流量全部重定向到 $indevice 处理
	tc qdisc add dev $outdevice handle ffff: ingress
	tc filter add dev $outdevice parent ffff: protocol ip u32 match u32 0 0 action mirred egress redirect dev $indevice
	 
	# $outdevice的出向限速:$outdevice添加根队列,使用htb,添加1:1类,使用htb 
	tc qdisc add dev $outdevice root handle 1: htb r2q 625 default 65
	tc class add dev $outdevice parent 1: classid 1:1 htb rate 1000Mbit
	 
	# $outdevice的入向限速:$indevice添加根队列,使用htb,添加1:1类,使用htb 
	tc qdisc add dev $indevice root handle 1: htb r2q 625 default 65
	tc class add dev $indevice parent 1: classid 1:1 htb rate 1000Mbit
	 
	# $outdevice的出向限速:$outdevice设备添加子类\对应的filter配置规则和子类的队列
	tc class add dev $outdevice parent 1:1 classid 1:10 htb rate 10Mbit
	tc filter add dev $outdevice parent 1: protocol all prio 1 u32 match ip dst 192.168.0.2 classid 1:10
	tc qdisc add dev $outdevice parent 1:10 handle 10: sfq
	 
	# $outdevice的出向限速:$outdevice设备添加子类\对应的filter配置规则和子类的队列 
	tc class add dev $outdevice parent 1:1 classid 1:11 htb rate 20Mbit
	tc filter add dev $outdevice parent 1: protocol all prio 1 u32 match ip dst 192.168.0.3 classid 1:11
	tc qdisc add dev $outdevice parent 1:11 handle 11: sfq
	 
	 
	# $outdevice的入向限速:$indevice设备添加子类\对应的filter配置规则和子类的队列
	tc class add dev $indevice parent 1:1 classid 1:10 htb rate 10Mbit
	tc filter add dev $indevice parent 1: protocol all prio 1 u32 match ip src 192.168.0.2 classid 1:10
	tc qdisc add dev $indevice parent 1:10 handle 10: sfq
	 
	 
	# $outdevice的入向限速:$indevice设备添加子类\对应的filter配置规则和子类的队列 
	tc class add dev $indevice parent 1:1 classid 1:11 htb rate 20Mbit
	tc filter add dev $indevice parent 1: protocol all prio 1 u32 match ip src 192.168.0.3 classid 1:11
	tc qdisc add dev $indevice parent 1:11 handle 11: sfq
```

### 网络压力测试

压力测试工具使用`iperf网络测试工具`

> 常用参数指南（详见附录）：
>
> -c/s：客户端模式/服务端模式
>
> -p：指定iperf测试端口
>
> -i：指定报告间隔
>
> -b：设置UDP的发送带宽，单位bit/s
>
> -t：设置测试的时长，单位为秒，不设置默认10s
>
> -l：指定包大小，TCP默认8k，UDP默认1470字节

```shell
# 针对TCP进行带宽性能测试
#服务端命令：
iperf -s -i 1 -p 3389
#
iperf -c 192.168.158.128 -p 3389 -i 1

#针对UDP进行带宽性能测试
#服务端命令
iperf -u -s -i 1 -p 3389
#客户端命令
iperf -u -c 192.168.158.128 -p 3389 -b 1500M -i 1 
# 参考资料：iperf网络测试工具， https://cloud.tencent.com/developer/article/1688469
```

带宽查看使用工具`iftop`

>     TX：发送流量
>     RX：接收流量
>     TOTAL：总流量
>     Cumm：运行iftop到目前时间的总流量
>     peak：流量峰值
>     rates：分别表示过去 2s 10s 40s 的平均流量

执行脚本前后，对于网络带宽进行检测，发现流量带宽明显受到TC的控制。实验成功。

## 总结

本文通过对Linux操作系统中`cgroup`、`tc`的使用，实现了对Linux操作系统中系统资源和网络资源的控制。本文的方法和工具在写作过程中可能还有用到，但是没有写进来的，欢迎大家留言或通过微信公众号联系我，我会尽力为大家解答。

## 其他补充

### 如何获取进程ID

#### 根据进程名称获取pid

 最简单的方法是使用 **pidof** 命令，用法：pidof  process_name，例如：

```shell
# 查看初始进程的pid
pidof init
# 
```

有事可能因为同一个名称的程序启动了多个进程，使用pidof时会返回多个pid，使用空格分开



#### 查找当前激活状态的网络链接及进程ID

```shell
lsof -i -n（不反向解析DNS）
# eg
lsof -i -n | awk   '{print $2, $9}'
# 输出为
#PID NAME
#29646 192.168.153.129:51044->13.250.177.223:https
#29646 192.168.153.129:52970->54.149.208.57:https

lsof -i -n -P  | awk '{print $1, $2,$5,$9}'
# 输出时，不解析端口，应该将https->443
```



## 参考资料：

Linux 资源隔离机制 -- CGroup https://zhuanlan.zhihu.com/p/47590418

[CGroup 介绍、应用实例及原理描述](https://www.cnblogs.com/caoxiaojian/p/5633430.html) https://www.cnblogs.com/caoxiaojian/p/5633430.html

流量控制 https://tonydeng.github.io/sdn-handbook/linux/tc.html

Traffic Control HOWTO https://tldp.org/HOWTO/Traffic-Control-HOWTO/

Linux TC(Traffic Control)框架原理解析， https://blog.csdn.net/dog250/article/details/40483627

 [linux下使用tc(Traffic Control) 流量控制命令模拟网络延迟和丢包](https://www.cnblogs.com/yulia/p/10346339.html)

TC流量控制 https://blog.csdn.net/who538592/article/details/79483323

